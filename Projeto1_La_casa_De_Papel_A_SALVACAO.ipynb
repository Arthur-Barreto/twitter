{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Projeto 1 - Ci√™ncia dos Dados"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Nome: Arthur Martins de Souza Barreto\n",
    "\n",
    "Nome: Giselle Vieira de Melo"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Aten√ß√£o: Ser√£o permitidos grupos de tr√™s pessoas, mas com uma rubrica mais exigente. Grupos deste tamanho precisar√£o fazer um question√°rio de avalia√ß√£o de trabalho em equipe"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "___\r\n",
    "\r\n",
    "Os g√™nios do crime que dominaram as telas do mundo inteiro em <em><b> La casa de Papel </b></em>  e que conquistaram v√°rias premia√ß√µes como o Emmy Internacional de melhor s√©rie dram√°tica, al√©m de se tornar uma das s√©ries mais populares da IMDb possui uma boa aclama√ß√£o cr√≠tica dado seu enredo sofisticado e dramas interpessoais, se tornando a s√©rie em l√≠ngua n√£o inglesa mais assistida de 2018. \r\n",
    "\r\n",
    "A partir disso, com o lan√ßamento da primeira parte da quinta (e √∫ltima) temporada no dia 03 de Setembro de 2021, √© de regular tend√™ncia pelo mundo inteiro coment√°rios no twitter, tendo at√© celebridades comentando sobre a s√©rie nessa rede. \r\n",
    "\r\n",
    "<center> <img src=\"imagens/Money-Heist.jpg\" width=500> <center> "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Dessa forma, por meio da categoriza√ß√£o dos postagens dos usu√°rios do twitter, esse projeto visa analisar os coment√°rios a respeito da obra, como elogios e cr√≠ticas diretamente relacionados √† s√©rie, desconsiderando por exemplo, tweets de marca√ß√£o ou sobre temas paralelos. \r\n",
    "\r\n",
    "Para tanto, o <em><b>Classificador Naive Bayes </b></em> foi utilizado!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "___\n",
    "Carregando algumas bibliotecas:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "# a biblioteca dos emojis precisa ser baixada, caso n√£o tenha baixado descomente a linha a baixo e fa√ßa o dowload\r\n",
    "#!pip install emoji \r\n",
    "# a biblioteca para a impress√£o de dados estatisticos precisa ser instalada, descomente caso n√£o tenha instalada \r\n",
    "#!pip install seaborn"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "source": [
    "%matplotlib inline\r\n",
    "import pandas as pd\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import numpy as np\r\n",
    "import os"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "source": [
    "# bibliotecas adicionais\r\n",
    "import re \r\n",
    "# para eliminar as preposi√ß√µes usei o link a seguir como referencia\r\n",
    "# https://stackoverflow.com/questions/29523254/python-remove-stop-words-from-pandas-dataframe\r\n",
    "import nltk \r\n",
    "# apra eliminar os emojis usei o seguinte link como referencia\r\n",
    "# https://stackoverflow.com/questions/33404752/removing-emojis-from-a-string-in-python\r\n",
    "import emoji\r\n",
    "\r\n",
    "# fun√ß√µes auxiliares para separar o emoji\r\n",
    "import functools\r\n",
    "import operator\r\n",
    "\r\n",
    "# fun√ß√£o aux para plot da matriz de porcentagem\r\n",
    "\r\n",
    "import seaborn as sn"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "source": [
    "print('Esperamos trabalhar no diret√≥rio')\r\n",
    "print(os.getcwd())"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Esperamos trabalhar no diret√≥rio\n",
      "c:\\Users\\gisel\\OneDrive\\Documentos\\INSPER\\Segundo Semestre\\C-DADOS\\PROJETOS\\twitter\n"
     ]
    }
   ],
   "metadata": {
    "scrolled": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Carregando a base de dados com os tweets classificados como relevantes e n√£o relevantes:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "source": [
    "filename = 'La casa de papel.xlsx'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "source": [
    "train = pd.read_excel(filename)\r\n",
    "train.head(5)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Treinamento</th>\n",
       "      <th>Relevante</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>coisas q ser√£o proibidas quando eu for preside...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>agr que t√¥ terminando de assistir la casa de p...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>paguem minha terapia (la casa de papel vc me p...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>to com pena de terminar la casa de papel</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>olha, quem me segue aqui sabe o tanto que odei...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Treinamento  Relevante\n",
       "0  coisas q ser√£o proibidas quando eu for preside...          1\n",
       "1  agr que t√¥ terminando de assistir la casa de p...          1\n",
       "2  paguem minha terapia (la casa de papel vc me p...          0\n",
       "3           to com pena de terminar la casa de papel          0\n",
       "4  olha, quem me segue aqui sabe o tanto que odei...          1"
      ]
     },
     "metadata": {},
     "execution_count": 38
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "source": [
    "test = pd.read_excel(filename, sheet_name = 'Teste')\r\n",
    "test.head(5)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Teste</th>\n",
       "      <th>Relevante</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>vou assistir a √∫ltima temperada de la casa de ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>s√≥ quero chegar em casa e me entupir de brigad...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>toda vez, a gnt depois que acaba de ver la cas...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@esteseverino na verdade eles se comunicavam p...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>n√£o t√¥ chorando horrores com o final de la cas...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Teste  Relevante\n",
       "0  vou assistir a √∫ltima temperada de la casa de ...          1\n",
       "1  s√≥ quero chegar em casa e me entupir de brigad...          1\n",
       "2  toda vez, a gnt depois que acaba de ver la cas...          0\n",
       "3  @esteseverino na verdade eles se comunicavam p...          1\n",
       "4  n√£o t√¥ chorando horrores com o final de la cas...          0"
      ]
     },
     "metadata": {},
     "execution_count": 39
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "___\n",
    "## Classificador autom√°tico de sentimento"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Nessa parte inicial do projeto √© necess√°rio categorizar a opini√£o daqueles coment√°rios no twitter relativos √† aprova√ß√£o da s√©rie, (mais precisamente da nova temporada).\r\n",
    "\r\n",
    "Assim, foram encontrados diversos tweets, sendo alguns destes demasiadamente vagos, onde n√£o dava para discernir se era um coment√°rio aprovando ou n√£o da s√©rie La casa de papel. Nesse caso, foi considerado relevante apenas os coment√°rios que deixavam claro a critica √† s√©rie. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- Fun√ß√µes utilizadas!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "source": [
    "# FUN√á√ïES AUXILIARES DO SISTEMA para a montagem da base de dados do treinamento\r\n",
    "def cleanup(text):\r\n",
    "    \"\"\"\r\n",
    "        Fun√ß√£o de limpeza muito simples que troca alguns sinais b√°sicos por espa√ßos\r\n",
    "    \"\"\"\r\n",
    "    #import string\r\n",
    "    punctuation = '[!-.:?;\\/)|,''\"\"‚Äú‚Äù@#(*]' # Note que os sinais [] s√£o delimitadores de um conjunto.\r\n",
    "    pattern = re.compile(punctuation)\r\n",
    "    text_subbed = re.sub(pattern, '', text)\r\n",
    "    return text_subbed.lower()\r\n",
    "\r\n",
    "# fun√ß√£o baseada no seguinte link: # https://stackoverflow.com/questions/49921720/how-to-split-emoji-from-each-other-python\r\n",
    "def separa_emoji(string):\r\n",
    "    em_split_emoji = emoji.get_emoji_regexp().split(string)\r\n",
    "    em_split_whitespace = [substr.split() for substr in em_split_emoji]\r\n",
    "    em_split = functools.reduce(operator.concat, em_split_whitespace)\r\n",
    "    return em_split\r\n",
    "\r\n",
    "# baixando as preposi√ß√µes\r\n",
    "nltk.download('stopwords')\r\n",
    "# definindo as preposi√ß√µes da lingua portuguesa\r\n",
    "stop = nltk.corpus.stopwords.words('portuguese')\r\n",
    "\r\n",
    "def segunda_limpeza(lis_tweet):\r\n",
    "    limpo = []\r\n",
    "    for pal in lis_tweet:\r\n",
    "        # al√©m de verificar se a palavra n√£o √© uma preposi√ß√£o vamos pegar s√≥ as que tem mais de 3 letras\r\n",
    "        # vamos aproveitar que estamos percorrendo cada palavra e separar os emojis nesse processo\r\n",
    "        if pal not in stop:\r\n",
    "            aux = separa_emoji(pal)\r\n",
    "            for info in aux:\r\n",
    "                limpo.append(info)\r\n",
    "        else:\r\n",
    "            pass\r\n",
    "    return limpo\r\n",
    "\r\n",
    "def frase_para_palavras(df):\r\n",
    "    '''\r\n",
    "    fun√ß√£o para percorrer linha a linha uma coluna do dataframe e concatenar as palavras em uma unica lista\r\n",
    "    para ser usada no value_counts()\r\n",
    "    '''\r\n",
    "    data = ''\r\n",
    "    \r\n",
    "    for tweet in df:\r\n",
    "        # o tweet √© a frase que a pessoa postou, que se encontra em cada linha da nossa coluna\r\n",
    "        titulo = 'la casa de papel'\r\n",
    "        titulo_new = 'lacasadepapel'\r\n",
    "        if titulo in tweet:\r\n",
    "            tweet = tweet.replace(titulo,titulo_new)\r\n",
    "        data += tweet\r\n",
    "        \r\n",
    "    # dando o split agora, para separar por virgula\r\n",
    "    data = data.split()\r\n",
    "    # data √© um vetor, podemos chamar a fun√ß√£o de tirar preposi√ß√£o aqui\r\n",
    "    data = segunda_limpeza(data)\r\n",
    "    return data"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\gisel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "___\n",
    "\n",
    "\n",
    "## Montando um Classificador Naive-Bayes\n",
    "\n",
    "Considerando apenas as mensagens da planilha Treinamento, ensine  seu classificador."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "O classificador Naive-Bayes √© da fam√≠lia de classificadores estat√≠sticos baseado no Teorema de Bayes, sendo utilizado para prever a ocorr√™ncia em um texto, muito √∫til para esse trabalho j√° que estamos tratando uma sequ√™ncia de tweets sobre o tema em quest√£o \"La casa de papel\". \r\n",
    "\r\n",
    "Assim, esse classificador, ao ter um conhecimento pr√©vio das condi√ß√µes - aqui dadas por relevante (1) e irrelevante (0) -  utiliza da frequ√™ncia de palavras para fazer sua classifica√ß√£o, j√° que ele trata cada palavra de uma frase como um termo independente, como por exemplo: \"la casa de papel √© muito bom\" √© equivalente a \"√© muito bom la casa de papel\".\r\n",
    "\r\n",
    "### O teorema de Bayes e as probabilidades independentes:\r\n",
    "\"Os eventos A e B s√£o independentes quando o fato de ter conhecimento sobre a ocorr√™ncia de A n√£o altera a expectativa sobre a probabilidade de ocorr√™ncia do evento B.\"\r\n",
    "\r\n",
    "Isso √© muito √∫til para o nosso modelo j√° que a probabilidade de que as mesmas frases sejam ditas em postagens distintas no twitter √© baixa, mas ao fazer a suposi√ß√£o de que cada palavra em uma frase atua de forma independente √© poss√≠vel se realizar os c√°lculos de probabilidade.\r\n",
    "\r\n",
    "### C√°lculos de probabilidade\r\n",
    "\r\n",
    "- legenda:\r\n",
    "\r\n",
    "    - $P(rel)$: probabilidade do post ser relevante;\r\n",
    "\r\n",
    "    - $P(irrel)$: probabilidade do post ser irrelevante;\r\n",
    "\r\n",
    "    - $P(total)$: $P(rel)$ + $P(irrel)$\r\n",
    "\r\n",
    "    - $P(rel|total)$: \r\n",
    "    $$P(rel|total) = \\frac{P(total|rel) P(rel)}{P(total)}$$\r\n",
    "    \r\n",
    "    - $P(irrel|total)$: \r\n",
    "    $$P(irrel|total) = \\frac{P(total|irrel) P(irrel)}{P(total)}$$\r\n",
    "\r\n",
    "#### Suaviza√ß√£o de Laplace\r\n",
    "Pelo c√°lculo das probabilidades independentes tem-se que, dado uma palavra, descobrir qual √© a probabilidade de essa palavra est√° na base de dados a partir de sua frequ√™ncia. Agora imagine que a palavra n√£o est√° na base de dados... o que ocorre √© que a probabilidade √© zero, e ao realizar a multiplica√ß√£o dos termos, o c√°lculo √© anulado, n√£o gerando nenhuma informa√ß√£o (o contr√°rio do que se espera).\r\n",
    "\r\n",
    "Assim, a suaviza√ß√£o de Laplace surge para prevenir esse caso, dado que √© adicionado 1 a cada contagem, para que o denominador nunca fique nulo. Em contrapartida, √© tamb√©m adicionado o n√∫mero de palavras totais ao divisor, o que leva a, ao realizar a divis√£o, gerar sempre um n√∫mero menor que um. \r\n",
    "\r\n",
    "\r\n",
    "\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Apresentado o medelo, vamos implementar agora no c√≥digo! "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "source": [
    "#convertendo em vari√°veis categ√≥ricas:\r\n",
    "train['Treinamento'] = train['Treinamento'].astype('category')\r\n",
    "test['Teste'] = test['Teste'].astype('category')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "source": [
    "#limpando pontua√ß√µes do dataframe\r\n",
    "train['Clean']=train['Treinamento'].apply(cleanup)\r\n",
    "test['Clean']=test['Teste'].apply(cleanup)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "source": [
    "test"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Teste</th>\n",
       "      <th>Relevante</th>\n",
       "      <th>Clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>vou assistir a √∫ltima temperada de la casa de ...</td>\n",
       "      <td>1</td>\n",
       "      <td>vou assistir a √∫ltima temperada de la casa de ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>s√≥ quero chegar em casa e me entupir de brigad...</td>\n",
       "      <td>1</td>\n",
       "      <td>s√≥ quero chegar em casa e me entupir de brigad...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>toda vez, a gnt depois que acaba de ver la cas...</td>\n",
       "      <td>0</td>\n",
       "      <td>toda vez a gnt depois que acaba de ver la casa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@esteseverino na verdade eles se comunicavam p...</td>\n",
       "      <td>1</td>\n",
       "      <td>esteseverino na verdade eles se comunicavam pe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>n√£o t√¥ chorando horrores com o final de la cas...</td>\n",
       "      <td>0</td>\n",
       "      <td>n√£o t√¥ chorando horrores com o final de la cas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>acabei la casa de papel</td>\n",
       "      <td>0</td>\n",
       "      <td>acabei la casa de papel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>precisamos conversar sobre o final dessa tempo...</td>\n",
       "      <td>0</td>\n",
       "      <td>precisamos conversar sobre o final dessa tempo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>eu odeio l√° casa de papel, mataram a t√≥quio va...</td>\n",
       "      <td>1</td>\n",
       "      <td>eu odeio l√° casa de papel mataram a t√≥quio vai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>pregui√ßa de ver essa parte 5 de la casa de papel</td>\n",
       "      <td>0</td>\n",
       "      <td>pregui√ßa de ver essa parte 5 de la casa de papel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>la casa de papel n√£o vai acabar nunca? n√£o agu...</td>\n",
       "      <td>1</td>\n",
       "      <td>la casa de papel n√£o vai acabar nunca n√£o ague...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Teste  Relevante  \\\n",
       "0    vou assistir a √∫ltima temperada de la casa de ...          1   \n",
       "1    s√≥ quero chegar em casa e me entupir de brigad...          1   \n",
       "2    toda vez, a gnt depois que acaba de ver la cas...          0   \n",
       "3    @esteseverino na verdade eles se comunicavam p...          1   \n",
       "4    n√£o t√¥ chorando horrores com o final de la cas...          0   \n",
       "..                                                 ...        ...   \n",
       "195                            acabei la casa de papel          0   \n",
       "196  precisamos conversar sobre o final dessa tempo...          0   \n",
       "197  eu odeio l√° casa de papel, mataram a t√≥quio va...          1   \n",
       "198   pregui√ßa de ver essa parte 5 de la casa de papel          0   \n",
       "199  la casa de papel n√£o vai acabar nunca? n√£o agu...          1   \n",
       "\n",
       "                                                 Clean  \n",
       "0    vou assistir a √∫ltima temperada de la casa de ...  \n",
       "1    s√≥ quero chegar em casa e me entupir de brigad...  \n",
       "2    toda vez a gnt depois que acaba de ver la casa...  \n",
       "3    esteseverino na verdade eles se comunicavam pe...  \n",
       "4    n√£o t√¥ chorando horrores com o final de la cas...  \n",
       "..                                                 ...  \n",
       "195                            acabei la casa de papel  \n",
       "196  precisamos conversar sobre o final dessa tempo...  \n",
       "197  eu odeio l√° casa de papel mataram a t√≥quio vai...  \n",
       "198   pregui√ßa de ver essa parte 5 de la casa de papel  \n",
       "199  la casa de papel n√£o vai acabar nunca n√£o ague...  \n",
       "\n",
       "[200 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "execution_count": 43
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "source": [
    "# post relevante\r\n",
    "post_rel = train.loc[(train['Relevante'] == 1)]\r\n",
    "# post irrelevante\r\n",
    "post_irrel = train.loc[(train['Relevante'] == 0)]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "source": [
    "# agora precisamos quebrar o texto em palavras, usando a fun√ß√£o aux que converter frase em palavras,\r\n",
    "# ao mesmo tempo, vamos retirar as preposi√ß√µes do c√≥digo, paralelamente a convers√£o, melhorando a base de dados, temos:\r\n",
    "data_rel   = frase_para_palavras(post_rel['Clean'])\r\n",
    "data_irrel = frase_para_palavras(post_irrel['Clean'])\r\n",
    "\r\n",
    "# criando a serie dos posts relevantes e irrelevantes\r\n",
    "serie_rel = pd.Series(data_rel)\r\n",
    "serie_irrel = pd.Series(data_irrel)\r\n",
    "# Formando uma base de daddos com todas as palavras nele:\r\n",
    "data_total = data_rel + data_irrel\r\n",
    "\r\n",
    "#criando a serie com a base total de dados\r\n",
    "serie_total = pd.Series(data_total)\r\n",
    "\r\n",
    "print(len(serie_rel))\r\n",
    "print(len(serie_irrel))\r\n",
    "print(len(serie_total))\r\n",
    "serie_irrel.tail(20)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1368\n",
      "1087\n",
      "2455\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1067        trabalhar\n",
       "1068               ü§¶üèæ\n",
       "1069               ü§¶üèæ\n",
       "1070           recebi\n",
       "1071          spoiler\n",
       "1072    lacasadepapel\n",
       "1073            quero\n",
       "1074          cometer\n",
       "1075     homic√≠diovou\n",
       "1076       aproveitar\n",
       "1077              pra\n",
       "1078              ver\n",
       "1079            novos\n",
       "1080        epis√≥dios\n",
       "1081               l√°\n",
       "1082             casa\n",
       "1083            papel\n",
       "1084       abafadinha\n",
       "1085               üôèüèº\n",
       "1086                ‚ú®\n",
       "dtype: object"
      ]
     },
     "metadata": {},
     "execution_count": 45
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Frequ√™ncias absolutas"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Palavras em um texto s√£o vari√°veis **qualitativas nominais**, portanto usaremos `value_counts()` para obter a tabela de frequ√™ncias relativas e absolutas:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "source": [
    "# construindo a tabela de relevante e irrelevante\r\n",
    "tabela_rel   = serie_rel.value_counts()\r\n",
    "tabela_irrel = serie_irrel.value_counts()\r\n",
    "tabela_total = serie_total.value_counts()\r\n",
    "lista_serie_total = serie_total.tolist()\r\n",
    "elementos_nao_repetidos = set(lista_serie_total)\r\n",
    "elementos_nao_repetidos = pd.DataFrame(elementos_nao_repetidos).value_counts()\r\n",
    "elementos_nao_repetidos"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "ü•∫                      1\n",
       "fdp                    1\n",
       "favs                   1\n",
       "faz                    1\n",
       "fazendo                1\n",
       "                      ..\n",
       "parece                 1\n",
       "parar                  1\n",
       "parab√©nslarissajmar    1\n",
       "papelm√≥                1\n",
       "100                    1\n",
       "Length: 1143, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 46
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "___\n",
    "### Verificando a performance do Classificador\n",
    "\n",
    "Agora voc√™ deve testar o seu classificador com a base de Testes."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "source": [
    "# postagens relevantes\r\n",
    "post_rel.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Treinamento</th>\n",
       "      <th>Relevante</th>\n",
       "      <th>Clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>coisas q ser√£o proibidas quando eu for preside...</td>\n",
       "      <td>1</td>\n",
       "      <td>coisas q ser√£o proibidas quando eu for preside...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>agr que t√¥ terminando de assistir la casa de p...</td>\n",
       "      <td>1</td>\n",
       "      <td>agr que t√¥ terminando de assistir la casa de p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>olha, quem me segue aqui sabe o tanto que odei...</td>\n",
       "      <td>1</td>\n",
       "      <td>olha quem me segue aqui sabe o tanto que odeio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>que final fdp do l√° casa de papel</td>\n",
       "      <td>1</td>\n",
       "      <td>que final fdp do l√° casa de papel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>final veio de l√° casa de papel, meu pai amado ...</td>\n",
       "      <td>1</td>\n",
       "      <td>final veio de l√° casa de papel meu pai amado o...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Treinamento  Relevante  \\\n",
       "0  coisas q ser√£o proibidas quando eu for preside...          1   \n",
       "1  agr que t√¥ terminando de assistir la casa de p...          1   \n",
       "4  olha, quem me segue aqui sabe o tanto que odei...          1   \n",
       "6                  que final fdp do l√° casa de papel          1   \n",
       "8  final veio de l√° casa de papel, meu pai amado ...          1   \n",
       "\n",
       "                                               Clean  \n",
       "0  coisas q ser√£o proibidas quando eu for preside...  \n",
       "1  agr que t√¥ terminando de assistir la casa de p...  \n",
       "4  olha quem me segue aqui sabe o tanto que odeio...  \n",
       "6                  que final fdp do l√° casa de papel  \n",
       "8  final veio de l√° casa de papel meu pai amado o...  "
      ]
     },
     "metadata": {},
     "execution_count": 47
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "source": [
    "# postagens irrelevantes\r\n",
    "post_irrel.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Treinamento</th>\n",
       "      <th>Relevante</th>\n",
       "      <th>Clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>paguem minha terapia (la casa de papel vc me p...</td>\n",
       "      <td>0</td>\n",
       "      <td>paguem minha terapia la casa de papel vc me paga</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>to com pena de terminar la casa de papel</td>\n",
       "      <td>0</td>\n",
       "      <td>to com pena de terminar la casa de papel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>simplesmente devastada com esse final da 1¬∞ pa...</td>\n",
       "      <td>0</td>\n",
       "      <td>simplesmente devastada com esse final da 1¬∞ pa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ai la casa de papel me mata pqp</td>\n",
       "      <td>0</td>\n",
       "      <td>ai la casa de papel me mata pqp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>criadores de l√° casa de papel deixaram mto na ...</td>\n",
       "      <td>0</td>\n",
       "      <td>criadores de l√° casa de papel deixaram mto na ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Treinamento  Relevante  \\\n",
       "2   paguem minha terapia (la casa de papel vc me p...          0   \n",
       "3            to com pena de terminar la casa de papel          0   \n",
       "5   simplesmente devastada com esse final da 1¬∞ pa...          0   \n",
       "7                     ai la casa de papel me mata pqp          0   \n",
       "10  criadores de l√° casa de papel deixaram mto na ...          0   \n",
       "\n",
       "                                                Clean  \n",
       "2    paguem minha terapia la casa de papel vc me paga  \n",
       "3            to com pena de terminar la casa de papel  \n",
       "5   simplesmente devastada com esse final da 1¬∞ pa...  \n",
       "7                     ai la casa de papel me mata pqp  \n",
       "10  criadores de l√° casa de papel deixaram mto na ...  "
      ]
     },
     "metadata": {},
     "execution_count": 48
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "source": [
    "p_rel = len(serie_rel)/len(serie_total)\r\n",
    "print('A probabilidade de relevantes √©: {0:.3f}'.format(p_rel))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "A probabilidade de relevantes √©: 0.557\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "source": [
    "p_irrel = len(serie_irrel)/len(serie_total)\r\n",
    "print('A probabilidade de irrelevate √©: {0:.3f}'.format(p_irrel))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "A probabilidade de irrelevate √©: 0.443\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Fun√ß√µes auxiliares para a suaviza√ß√£o de la place"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "source": [
    "def quantas_vezes_aparece_palavra(palavra,tabela):\r\n",
    "    # a fun√ß√£o retorna o valor de vezes que a palavra aparece no seu conjunto\r\n",
    "    # por exemplo, quantas vezes aparece na tabela relevante?\r\n",
    "    # fazemos: num = tabela_rel['palavra']\r\n",
    "    # e retornamos esse valor\r\n",
    "    # por outro lado, pode n√£o ter a palavra, por isso a suavia√ß√£o de la place, quando isso ocorrer,\r\n",
    "    # vamos retornar 0\r\n",
    "    try:\r\n",
    "        # caso em que a palavra tem na tabela de frequencias\r\n",
    "        quant = tabela[palavra]\r\n",
    "        return quant\r\n",
    "    except:\r\n",
    "        # aqui a palavra n√£o est√° na tabela de frequencias,retornamos 0\r\n",
    "        return 0\r\n",
    "    \r\n",
    "def laplace_smoothing(palavras_da_classe,quantidade):\r\n",
    "    # vamos aplicar a formula da suaviza√ß√£o\r\n",
    "    num = quantidade + 1\r\n",
    "    den = len(palavras_da_classe) + len(elementos_nao_repetidos)\r\n",
    "    #den = 2193 + len(elementos_nao_repetidos)\r\n",
    "    laplace = num/den\r\n",
    "    return laplace\r\n",
    "\r\n",
    "def p_tweet(tweet,relevante):\r\n",
    "    # se relevante for true, calcula para relevante\r\n",
    "    # se for false, para irrelevante\r\n",
    "    p = 1\r\n",
    "    # quebrando o tweet a cada espa√ßo \r\n",
    "    tweet_em_palavras = tweet.split()\r\n",
    "    \r\n",
    "    for palavra in tweet_em_palavras:\r\n",
    "        # vamos calcular o valor da frequencia absoluta da palavra\r\n",
    "        # para isto, usaremos a funcao quantas_vezes_aparece_palavra\r\n",
    "        # verificando qual classe queremos calcular p\r\n",
    "        \r\n",
    "        if relevante:\r\n",
    "            quant = quantas_vezes_aparece_palavra(palavra,tabela_rel)\r\n",
    "            p_palavra = laplace_smoothing(serie_rel,quant)\r\n",
    "            \r\n",
    "        else:\r\n",
    "            quant = quantas_vezes_aparece_palavra(palavra,tabela_irrel)\r\n",
    "            p_palavra = laplace_smoothing(serie_irrel,quant)\r\n",
    "            \r\n",
    "        p *= p_palavra\r\n",
    "    return p\r\n",
    "\r\n",
    "def rel_ou_irrel(tweet):\r\n",
    "    # vamos decidir se o tweet √© relevante ou n√£o\r\n",
    "    # para tal vamos aplicar o teorema de bayes\r\n",
    "    P_tweet_rel   = p_tweet(tweet,True)\r\n",
    "    P_tweet_irrel = p_tweet(tweet,False)\r\n",
    "    rel_bayes     = P_tweet_rel*p_rel\r\n",
    "    irrel_bayes   = P_tweet_irrel*p_irrel\r\n",
    "    #print(rel_bayes,irrel_bayes)\r\n",
    "    # vamos usar o mesmo criterio adotado na tabela, 1 para relevante e 0 para irrelevante\r\n",
    "    if rel_bayes > irrel_bayes:\r\n",
    "        return 1\r\n",
    "    else:\r\n",
    "        return 0"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "source": [
    "# agora vamos criar uma coluna nova, com a classifica√ß√£o dos tweets\r\n",
    "train['Classifica√ß√£o'] = train['Clean'].apply(rel_ou_irrel)\r\n",
    "train"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Treinamento</th>\n",
       "      <th>Relevante</th>\n",
       "      <th>Clean</th>\n",
       "      <th>Classifica√ß√£o</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>coisas q ser√£o proibidas quando eu for preside...</td>\n",
       "      <td>1</td>\n",
       "      <td>coisas q ser√£o proibidas quando eu for preside...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>agr que t√¥ terminando de assistir la casa de p...</td>\n",
       "      <td>1</td>\n",
       "      <td>agr que t√¥ terminando de assistir la casa de p...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>paguem minha terapia (la casa de papel vc me p...</td>\n",
       "      <td>0</td>\n",
       "      <td>paguem minha terapia la casa de papel vc me paga</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>to com pena de terminar la casa de papel</td>\n",
       "      <td>0</td>\n",
       "      <td>to com pena de terminar la casa de papel</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>olha, quem me segue aqui sabe o tanto que odei...</td>\n",
       "      <td>1</td>\n",
       "      <td>olha quem me segue aqui sabe o tanto que odeio...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>m√≥ sacanagem esse final de la casa de papel</td>\n",
       "      <td>1</td>\n",
       "      <td>m√≥ sacanagem esse final de la casa de papel</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>recebi spoiler de la casa de papel e quero com...</td>\n",
       "      <td>0</td>\n",
       "      <td>recebi spoiler de la casa de papel e quero com...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>terminei la casa de papel e me tornei o homem ...</td>\n",
       "      <td>1</td>\n",
       "      <td>terminei la casa de papel e me tornei o homem ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>vou aproveitar pra ver os novos epis√≥dios de l...</td>\n",
       "      <td>0</td>\n",
       "      <td>vou aproveitar pra ver os novos epis√≥dios de l...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>terminei de assistir l√° casa de papel ,algu√©m ...</td>\n",
       "      <td>1</td>\n",
       "      <td>terminei de assistir l√° casa de papel algu√©m p...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>300 rows √ó 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Treinamento  Relevante  \\\n",
       "0    coisas q ser√£o proibidas quando eu for preside...          1   \n",
       "1    agr que t√¥ terminando de assistir la casa de p...          1   \n",
       "2    paguem minha terapia (la casa de papel vc me p...          0   \n",
       "3             to com pena de terminar la casa de papel          0   \n",
       "4    olha, quem me segue aqui sabe o tanto que odei...          1   \n",
       "..                                                 ...        ...   \n",
       "295        m√≥ sacanagem esse final de la casa de papel          1   \n",
       "296  recebi spoiler de la casa de papel e quero com...          0   \n",
       "297  terminei la casa de papel e me tornei o homem ...          1   \n",
       "298  vou aproveitar pra ver os novos epis√≥dios de l...          0   \n",
       "299  terminei de assistir l√° casa de papel ,algu√©m ...          1   \n",
       "\n",
       "                                                 Clean  Classifica√ß√£o  \n",
       "0    coisas q ser√£o proibidas quando eu for preside...              1  \n",
       "1    agr que t√¥ terminando de assistir la casa de p...              1  \n",
       "2     paguem minha terapia la casa de papel vc me paga              0  \n",
       "3             to com pena de terminar la casa de papel              0  \n",
       "4    olha quem me segue aqui sabe o tanto que odeio...              1  \n",
       "..                                                 ...            ...  \n",
       "295        m√≥ sacanagem esse final de la casa de papel              1  \n",
       "296  recebi spoiler de la casa de papel e quero com...              0  \n",
       "297  terminei la casa de papel e me tornei o homem ...              1  \n",
       "298  vou aproveitar pra ver os novos epis√≥dios de l...              0  \n",
       "299  terminei de assistir l√° casa de papel algu√©m p...              1  \n",
       "\n",
       "[300 rows x 4 columns]"
      ]
     },
     "metadata": {},
     "execution_count": 52
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "source": [
    "test['Classifica√ß√£o'] = test['Clean'].apply(rel_ou_irrel)\r\n",
    "test"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Teste</th>\n",
       "      <th>Relevante</th>\n",
       "      <th>Clean</th>\n",
       "      <th>Classifica√ß√£o</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>vou assistir a √∫ltima temperada de la casa de ...</td>\n",
       "      <td>1</td>\n",
       "      <td>vou assistir a √∫ltima temperada de la casa de ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>s√≥ quero chegar em casa e me entupir de brigad...</td>\n",
       "      <td>1</td>\n",
       "      <td>s√≥ quero chegar em casa e me entupir de brigad...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>toda vez, a gnt depois que acaba de ver la cas...</td>\n",
       "      <td>0</td>\n",
       "      <td>toda vez a gnt depois que acaba de ver la casa...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@esteseverino na verdade eles se comunicavam p...</td>\n",
       "      <td>1</td>\n",
       "      <td>esteseverino na verdade eles se comunicavam pe...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>n√£o t√¥ chorando horrores com o final de la cas...</td>\n",
       "      <td>0</td>\n",
       "      <td>n√£o t√¥ chorando horrores com o final de la cas...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>acabei la casa de papel</td>\n",
       "      <td>0</td>\n",
       "      <td>acabei la casa de papel</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>precisamos conversar sobre o final dessa tempo...</td>\n",
       "      <td>0</td>\n",
       "      <td>precisamos conversar sobre o final dessa tempo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>eu odeio l√° casa de papel, mataram a t√≥quio va...</td>\n",
       "      <td>1</td>\n",
       "      <td>eu odeio l√° casa de papel mataram a t√≥quio vai...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>pregui√ßa de ver essa parte 5 de la casa de papel</td>\n",
       "      <td>0</td>\n",
       "      <td>pregui√ßa de ver essa parte 5 de la casa de papel</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>la casa de papel n√£o vai acabar nunca? n√£o agu...</td>\n",
       "      <td>1</td>\n",
       "      <td>la casa de papel n√£o vai acabar nunca n√£o ague...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows √ó 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Teste  Relevante  \\\n",
       "0    vou assistir a √∫ltima temperada de la casa de ...          1   \n",
       "1    s√≥ quero chegar em casa e me entupir de brigad...          1   \n",
       "2    toda vez, a gnt depois que acaba de ver la cas...          0   \n",
       "3    @esteseverino na verdade eles se comunicavam p...          1   \n",
       "4    n√£o t√¥ chorando horrores com o final de la cas...          0   \n",
       "..                                                 ...        ...   \n",
       "195                            acabei la casa de papel          0   \n",
       "196  precisamos conversar sobre o final dessa tempo...          0   \n",
       "197  eu odeio l√° casa de papel, mataram a t√≥quio va...          1   \n",
       "198   pregui√ßa de ver essa parte 5 de la casa de papel          0   \n",
       "199  la casa de papel n√£o vai acabar nunca? n√£o agu...          1   \n",
       "\n",
       "                                                 Clean  Classifica√ß√£o  \n",
       "0    vou assistir a √∫ltima temperada de la casa de ...              1  \n",
       "1    s√≥ quero chegar em casa e me entupir de brigad...              0  \n",
       "2    toda vez a gnt depois que acaba de ver la casa...              0  \n",
       "3    esteseverino na verdade eles se comunicavam pe...              0  \n",
       "4    n√£o t√¥ chorando horrores com o final de la cas...              1  \n",
       "..                                                 ...            ...  \n",
       "195                            acabei la casa de papel              1  \n",
       "196  precisamos conversar sobre o final dessa tempo...              1  \n",
       "197  eu odeio l√° casa de papel mataram a t√≥quio vai...              1  \n",
       "198   pregui√ßa de ver essa parte 5 de la casa de papel              0  \n",
       "199  la casa de papel n√£o vai acabar nunca n√£o ague...              0  \n",
       "\n",
       "[200 rows x 4 columns]"
      ]
     },
     "metadata": {},
     "execution_count": 53
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "source": [
    "# verdadeiro_negativo = test.loc[(test['Relevante']==0)&(test['Classifica√ß√£o']==0),:].shape[0]\r\n",
    "# verdadeiro_positivo = test.loc[(test['Relevante']==1)&(test['Classifica√ß√£o']==1),:].shape[0]\r\n",
    "# acuracia = (verdadeiro_positivo + verdadeiro_negativo)/test.shape[0]\r\n",
    "# print('A acuracia do classificador na base de teste foi de {:.2f} %'.format(acuracia*100))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "source": [
    "# # analizando a classifica√ßao como um todo, e poltando um grafico, temos:\r\n",
    "# df_cm=pd.crosstab(test.Relevante,test.Classifica√ß√£o,normalize=True)\r\n",
    "# plt.figure(figsize = (9,7))\r\n",
    "# plt.title('Matriz de Confus√£o na Base Treino \\n informa√ß√µes em porcentagem\\n',fontdict={'fontsize': 14})\r\n",
    "# sn.heatmap(df_cm, annot=True, annot_kws={\"size\":30},fmt='.1%',cmap=\"BuPu\")"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Deste modo, temos: \n",
    "14   % de falso positivo\n",
    "16,5 % de verdadeiro positivo\n",
    "23   % de falso negativo\n",
    "46,5 % de verdadeiro negativo"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "source": [
    "#porcentagem de verdadeiros negativos:\r\n",
    "relevante = test.loc[test['Relevante']==0]\r\n",
    "classificacao_rel = relevante.loc[relevante['Classifica√ß√£o']==0]\r\n",
    "#print('total de verdadeiros negativos:{0}'.format(len(classificacao_rel)))\r\n",
    "print('A porcentagem de verdadeiros negativos: {:.4}%'.format( ((len(classificacao_rel))/len(test['Relevante'])*100)))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "A porcentagem de verdadeiros negativos: 46.0%\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "source": [
    "#porcentagem de verdadeiros positivos:\r\n",
    "irrelevante = test.loc[test['Relevante']==1]\r\n",
    "classificacao_irrel = irrelevante.loc[irrelevante['Classifica√ß√£o']==1]\r\n",
    "\r\n",
    "print('A porcentagem de verdadeiros positivos: {:.4}%'.format( ((len(classificacao_irrel))/len(test['Relevante'])*100)))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "A porcentagem de verdadeiros positivos: 17.5%\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "source": [
    "#porcentagem de falsos negativos:\r\n",
    "relevante = test.loc[test['Relevante']==1]\r\n",
    "classificacao_irrel = relevante.loc[relevante['Classifica√ß√£o']==0]\r\n",
    "print('A porcentagem de falsos negativos √©: {:.4}%'.format(((len(classificacao_irrel))/len(test['Relevante'])*100)))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "A porcentagem de falsos negativos √©: 13.0%\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "source": [
    "#porcentagem de falsos positivo:\r\n",
    "irrelevante = test.loc[test['Relevante']==0]\r\n",
    "classificacao_rel = irrelevante.loc[irrelevante['Classifica√ß√£o']==1]\r\n",
    "print('a porcentagem de falsos positivos √©: {:.4}%'.format( ((len(classificacao_rel))/len(test['Relevante'])*100)))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "a porcentagem de falsos positivos √©: 23.5%\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "source": [
    "#acur√°cia\r\n",
    "total_irrel = len(test.loc[(test['Relevante']==0)&(test['Classifica√ß√£o']==0)])\r\n",
    "total_rel = len(test.loc[(test['Relevante']==1)&(test['Classifica√ß√£o']==1)])\r\n",
    "\r\n",
    "total = total_rel + total_irrel\r\n",
    "acuracia = total/len(test['Relevante'])\r\n",
    "acuracia *= 100\r\n",
    "print('A acur√°cia √©: {:.4} % '.format(acuracia))\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "A acur√°cia √©: 63.5 % \n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "source": [
    "#Vamos fazer a acur√°cia da base de dados de treinamento para an√°lises conclusivas:\r\n",
    "total_irrel_train = len(train.loc[(train['Relevante']==0)&(train['Classifica√ß√£o']==0)])\r\n",
    "total_rel_train = len(train.loc[(train['Relevante']==1)&(train['Classifica√ß√£o']==1)])\r\n",
    "\r\n",
    "total_train = total_rel_train + total_irrel_train\r\n",
    "acuracia_train = total/len(train['Relevante'])\r\n",
    "acuracia_train *= 100\r\n",
    "print('A acur√°cia √©: {:.4} % '.format(acuracia_train))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "A acur√°cia √©: 42.33 % \n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "___\r\n",
    "## Concluindo"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Interpreta√ß√µes do modelo:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "A partir da montagem do classificador √© poss√≠vel ent√£o fazer an√°lises sobre a execu√ß√£o do nosso modelo probabil√≠stico. Assim, com uma acur√°cia de 63.5% pode-se interpretar que ele teve um desempenho razo√°vel, por mais que ainda insatisfat√≥rio dados que ainda representa menos de 75% de acerto. Isso significa que, ap√≥s ele ter feito a sua classifica√ß√£o, quando comparado com a base de dados classificada manualmente, ele obteve 63.5% de tweets com classifica√ß√µes coerentes com a realidade.  \r\n",
    "\r\n",
    "Isso se deve, sobretudo, a classifica√ß√£o dos tweets falsos positivos (aqueles que foram demarcados como relevantes, mas que na verdade s√£o irrelevantes), sendo 23.5% em nosso modelo e dos falsos negativos (aqueles demarcados como irrelevantes, mas que na verdade s√£o relevantes), com 13% em nosso modelo. \r\n",
    "\r\n",
    "Uma forma de explicar isso √© pelo fato de se utilizar probabilidades independentes para classifica√ß√£o. Ou seja, os tweets s√£o classificados apenas pela probabilidade independente de cada palavra ser classificada como relevante ou irrelevante, ignorando a interpreta√ß√£o das frases como um todo, deixando de lado o seu sentido. Assim, frases ironicas, sarc√°sticas, amb√≠guas ou com dupla nega√ß√£o s√£o um risco para o nosso modelo. \r\n",
    "\r\n",
    "Apesar disso, o tamanho da base de dados influencia diretamente na qualidade de desempenho do classificador, j√° que quanto mais postagens armazenadas na base de dados, maior √© a assimila√ß√£o de palavras e maior √© a quantidade de palavras que podem ser usadas em postagens posteriores. Dado isso, voc√™ pode se perguntar <em><b> \"Seria interessante ent√£o utilizar o pr√≥prio classificador para gerar mais amostras de treinamento\" </em></b> e a resposta para isso √© <em><b>\"n√£o\"</em></b>, de forma direta, o algor√≠tmo toma como base a certeza de que as classifica√ß√µes da base de dados de treinamento √© 100% correta e constr√≥i o seu classificador (que no caso do projeto possui uma probabilidade de errar em 36,5% das cassifica√ß√µes)  assim, se para futuras classifica√ß√µes for utilizado tweets classificados erroneamente, ele far√° classifica√ß√µes cada vez mais erradas. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Onde √© poss√≠vel aplicar o Na√Øve Bayes fora do contexto desse projeto?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Tendo em vista "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "De acordo com o artigo \"Bazinga! Caracterizando e Detectando Sarcasmo e Ironia no Twitter\" </em></b>  da Universidade Federal de Minas gerais, percebemos que o tweet contendo sarcasmo faz uso significativo de palavras que expressam concord√¢ncia, ainda que de maneira mais informal, com uso de express√µes como ‚ÄúEr‚Äù, ‚Äúhm‚Äùe ‚Äúumm‚Äù. Outro aspecto observado est√° relacionado ao uso intenso da pontuac¬∏ Àúao dois pontos (‚Äú:‚Äù), que √© utilizada para o an√∫ncio ou introdu√ß√£o de um esclarecimento ou cita√ß√£o. J√° o conte√∫do ir√¥nico tem presen√ßa significativa de pronomes na terceira pessoa do plural. Estudos anteriores indicam que essas s√£o caracter√≠sticas de uma linguagem mais informal."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<center> <img src=\"imagens/Tabela de ironia.png\" width=500> <center>\r\n",
    "foto retirada do artigo <em><b> \"Bazinga! Caracterizando e Detectando Sarcasmo e Ironia no Twitter\" </em></b>  da Universidade Federal de Minas gerais"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "___\n",
    "### Qualidade do Classificador a partir de novas separa√ß√µes dos tweets entre Treinamento e Teste\n",
    "\n",
    "Caso for fazer esse item do Projeto"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "source": [
    "# vamos juntar as duas tabelas em uma tabela s√≥\r\n",
    "test_novo = pd.read_excel(filename, sheet_name = 'Teste')\r\n",
    "test_novo = test_novo.rename(columns={'Teste':'Total'})\r\n",
    "\r\n",
    "train_novo = pd.read_excel(filename, sheet_name = 'Treinamento')\r\n",
    "train_novo = train_novo.rename(columns={'Treinamento':'Total'})\r\n",
    "\r\n",
    "# juntando as duas tabelas\r\n",
    "data_total_ = pd.concat([train_novo,test_novo])\r\n",
    "# limpando a pontua√ß√£o\r\n",
    "# a limpeza mais profunda ocorrer√° quando transfprmamos os tweets em palavras, neste passo, iremos limpar, separar emojis, tudo\r\n",
    "# paralelamente\r\n",
    "data_total_['Clean'] = data_total_['Total'].apply(cleanup)\r\n",
    "data_total_\r\n",
    "type(data_total_)\r\n",
    "# agora devemos aplicar a rotina de limpeza novamente"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "metadata": {},
     "execution_count": 62
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "source": [
    "# https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\r\n",
    "# vamos usar a biblioteca sklearn.model_selection.train_test_split, com c√≥digo de exemplo obtido no link acima\r\n",
    "\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "# vamos guardar os verdadeiros positivos, verdadeiros negativos e a acuracia de cada rodagem em um vetor para analiza grafica futura\r\n",
    "true_positive = []\r\n",
    "true_negative = []\r\n",
    "acurracy      = []\r\n",
    "\r\n",
    "# queremos repetir pelo menos 100 vezes, para repetir n vezes coloque no segundo argumento no range n-1\r\n",
    "for i in range(0,100):\r\n",
    "    #print(type(data_total))\r\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data_total_[['Clean','Relevante']],data_total_['Relevante'], test_size=0.4)\r\n",
    "    # teste size √© a propor√ß√£o da base de dados para teste. Como queremos 200/500, teste sieze ser√° 0.4\r\n",
    "    #print('amigo estou aq')\r\n",
    "    # separando o treinamento relevante do irrelevante\r\n",
    "    train_relevantes   = X_train[X_train['Relevante'] == 1]\r\n",
    "    train_irrelevantes = X_train[X_train['Relevante'] == 0]\r\n",
    "    \r\n",
    "    # agora vamos quebrar o texto em palavras, e fazer a segunda limpeza nos dados, desde considerar 'la casa de papel'\r\n",
    "    # como 'lacasadepapel' a separar os emojis. A FUN√á√ÉO FRASE_PARA_PALAVRAS fara a limpeza citada\r\n",
    "    \r\n",
    "    data_rel   = frase_para_palavras(train_relevantes['Clean'])\r\n",
    "    data_irrel = frase_para_palavras(train_irrelevantes['Clean'])\r\n",
    "    \r\n",
    "    # Formando uma base de daddos com todas as palavras nele:\r\n",
    "    data_total = data_rel + data_irrel\r\n",
    "    \r\n",
    "    # criando a serie dos posts relevantes e irrelevantes\r\n",
    "    serie_rel = pd.Series(data_rel)\r\n",
    "    serie_irrel = pd.Series(data_irrel)\r\n",
    "    #criando a serie com a base total de dados\r\n",
    "    serie_total = pd.Series(data_total)\r\n",
    "    \r\n",
    "    # criando tabela de frequencia das palavras, seram usadas para calcular a frequencia absoluta na suaviza√ß√£o de laplace\r\n",
    "    tabela_rel   = serie_rel.value_counts()\r\n",
    "    tabela_irrel = serie_irrel.value_counts()\r\n",
    "    tabela_total = serie_total.value_counts()\r\n",
    "    # vamos calcular os elementos n√£o repetidos, necess√°rio na suaviza√ß√£o de laplace\r\n",
    "    lista_serie_total = serie_total.tolist()\r\n",
    "    elementos_nao_repetidos = set(lista_serie_total)\r\n",
    "    elementos_nao_repetidos = pd.DataFrame(elementos_nao_repetidos).value_counts()\r\n",
    "    \r\n",
    "    # definindo a probabilidade de ser relevante e a probabilidade de ser irrelevante\r\n",
    "    p_rel = len(serie_rel)/len(serie_total)\r\n",
    "    p_irrel = len(serie_irrel)/len(serie_total)\r\n",
    "    \r\n",
    "    # neste momento vamos classificar a base de teste\r\n",
    "    X_test['Classifica√ß√£o'] = X_test['Clean'].apply(rel_ou_irrel)\r\n",
    "    \r\n",
    "    # agora vamos calular porcentagem dos verdadeiros positivos, verdadeiros negativos e a acuracia do sistema\r\n",
    "    verdadeiro_positivo=X_test.loc[(X_test['Classifica√ß√£o']==1)&(X_test['Relevante']==1),:].shape[0]\r\n",
    "    verdadeiro_negativo=X_test.loc[(X_test['Classifica√ß√£o']==0)&(X_test['Relevante']==0),:].shape[0]\r\n",
    "    acuracia=(verdadeiro_positivo+verdadeiro_negativo)/X_test.shape[0]\r\n",
    "    \r\n",
    "    # calculando as porcentagens:\r\n",
    "    P_ver_pos = (verdadeiro_positivo/X_test.shape[0])*100\r\n",
    "    P_ver_neg = (verdadeiro_negativo/X_test.shape[0])*100\r\n",
    "    acuracia *= 100\r\n",
    "    \r\n",
    "    # guardando os verdadeiros positivos, negativos e a acuraria no vetor vazia criando antes do la√ßo de repeti√ß√£o\r\n",
    "    true_positive.append(P_ver_pos)\r\n",
    "    true_negative.append(P_ver_neg)\r\n",
    "    acurracy.append(acuracia)    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "source": [
    "faixa=np.arange(30,99,1)\r\n",
    "plt.figure(figsize=(10, 5))\r\n",
    "plt.hist(acurracy, bins=faixa, edgecolor='black', density=False)\r\n",
    "plt.title('Frequencia dos scores do modelo')\r\n",
    "plt.ylabel('Frequ√™ncia')\r\n",
    "plt.xlabel('Porcentagem de acerto (%)')\r\n",
    "plt.xlim(52.5,72.5)\r\n",
    "plt.show()"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "___\n",
    "## Aperfei√ßoamento:\n",
    "\n",
    "Trabalhos que conseguirem pelo menos conceito B v√£o evoluir em conceito dependendo da quantidade de itens avan√ßados:\n",
    "\n",
    "* IMPLEMENTOU outras limpezas e transforma√ß√µes que n√£o afetem a qualidade da informa√ß√£o contida nos tweets. Ex: stemming, lemmatization, stopwords\n",
    "* CORRIGIU separa√ß√£o de espa√ßos entre palavras e emojis ou entre emojis e emojis\n",
    "* CRIOU categorias intermedi√°rias de relev√¢ncia baseadas na probabilidade: ex.: muito relevante, relevante, neutro, irrelevante, muito irrelevante. Pelo menos quatro categorias, com adi√ß√£o de mais tweets na base, conforme enunciado. (OBRIGAT√ìRIO PARA TRIOS, sem contar como item avan√ßado)\n",
    "* EXPLICOU porqu√™ n√£o pode usar o pr√≥prio classificador para gerar mais amostras de treinamento\n",
    "* PROP√îS diferentes cen√°rios para Na√Øve Bayes fora do contexto do projeto\n",
    "* SUGERIU e EXPLICOU melhorias reais com indica√ß√µes concretas de como implementar (indicar como fazer e indicar material de pesquisa)\n",
    "* FEZ o item 6. Qualidade do Classificador a partir de novas separa√ß√µes dos tweets entre Treinamento e Teste descrito no enunciado do projeto (OBRIGAT√ìRIO para conceitos A ou A+)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "___\n",
    "## Refer√™ncias"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "[Naive Bayes and Text Classification](https://arxiv.org/pdf/1410.5329.pdf)  **Mais completo**\n",
    "\n",
    "[A practical explanation of a Naive Bayes Classifier](https://monkeylearn.com/blog/practical-explanation-naive-bayes-classifier/) **Mais simples**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a036e081e21c160afc26bb4b03f978afee4695dfefad222ea89752b41a5783e0"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}