{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Projeto 1 - Ciência dos Dados"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Nome: Arthur Martins de Souza Barreto\n",
    "\n",
    "Nome: Giselle Vieira de Melo"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Atenção: Serão permitidos grupos de três pessoas, mas com uma rubrica mais exigente. Grupos deste tamanho precisarão fazer um questionário de avaliação de trabalho em equipe"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "___\r\n",
    "\r\n",
    "Os gênios do crime que dominaram as telas do mundo inteiro em <em><b> La casa de Papel </b></em>  e que conquistaram várias premiações como o Emmy Internacional de melhor série dramática, além de se tornar uma das séries mais populares da IMDb possui uma boa aclamação crítica dado seu enredo sofisticado e dramas interpessoais, se tornando a série em língua não inglesa mais assistida de 2018. \r\n",
    "\r\n",
    "A partir disso, com o lançamento da primeira parte da quinta (e última) temporada no dia 03 de Setembro de 2021, é de regular tendência pelo mundo inteiro comentários no twitter, tendo até celebridades comentando sobre a série nessa rede. \r\n",
    "\r\n",
    "<center> <img src=\"imagens/Money-Heist.jpg\" width=500> <center> "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Dessa forma, por meio da categorização dos postagens dos usuários do twitter, esse projeto visa analisar os comentários a respeito da obra, como elogios e críticas diretamente relacionados à série, desconsiderando por exemplo, tweets de marcação ou sobre temas paralelos. \r\n",
    "\r\n",
    "Para tanto, o <em><b>Classificador Naive Bayes </b></em> foi utilizado!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "___\n",
    "Carregando algumas bibliotecas:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "# a biblioteca dos emojis precisa ser baixada, caso não tenha baixado descomente a linha a baixo e faça o dowload\r\n",
    "#!pip install emoji \r\n",
    "# a biblioteca para a impressão de dados estatisticos precisa ser instalada, descomente caso não tenha instalada \r\n",
    "#!pip install seaborn"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "source": [
    "%matplotlib inline\r\n",
    "import pandas as pd\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import numpy as np\r\n",
    "import os"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "source": [
    "# bibliotecas adicionais\r\n",
    "import re \r\n",
    "# para eliminar as preposições usei o link a seguir como referencia\r\n",
    "# https://stackoverflow.com/questions/29523254/python-remove-stop-words-from-pandas-dataframe\r\n",
    "import nltk \r\n",
    "# apra eliminar os emojis usei o seguinte link como referencia\r\n",
    "# https://stackoverflow.com/questions/33404752/removing-emojis-from-a-string-in-python\r\n",
    "import emoji\r\n",
    "\r\n",
    "# funções auxiliares para separar o emoji\r\n",
    "import functools\r\n",
    "import operator\r\n",
    "\r\n",
    "# função aux para plot da matriz de porcentagem\r\n",
    "\r\n",
    "import seaborn as sn"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "source": [
    "print('Esperamos trabalhar no diretório')\r\n",
    "print(os.getcwd())"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Esperamos trabalhar no diretório\n",
      "c:\\Users\\gisel\\OneDrive\\Documentos\\INSPER\\Segundo Semestre\\C-DADOS\\PROJETOS\\twitter\n"
     ]
    }
   ],
   "metadata": {
    "scrolled": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Carregando a base de dados com os tweets classificados como relevantes e não relevantes:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "source": [
    "filename = 'La casa de papel.xlsx'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "source": [
    "train = pd.read_excel(filename)\r\n",
    "train.head(5)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Treinamento</th>\n",
       "      <th>Relevante</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>coisas q serão proibidas quando eu for preside...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>agr que tô terminando de assistir la casa de p...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>paguem minha terapia (la casa de papel vc me p...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>to com pena de terminar la casa de papel</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>olha, quem me segue aqui sabe o tanto que odei...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Treinamento  Relevante\n",
       "0  coisas q serão proibidas quando eu for preside...          1\n",
       "1  agr que tô terminando de assistir la casa de p...          1\n",
       "2  paguem minha terapia (la casa de papel vc me p...          0\n",
       "3           to com pena de terminar la casa de papel          0\n",
       "4  olha, quem me segue aqui sabe o tanto que odei...          1"
      ]
     },
     "metadata": {},
     "execution_count": 38
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "source": [
    "test = pd.read_excel(filename, sheet_name = 'Teste')\r\n",
    "test.head(5)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Teste</th>\n",
       "      <th>Relevante</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>vou assistir a última temperada de la casa de ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>só quero chegar em casa e me entupir de brigad...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>toda vez, a gnt depois que acaba de ver la cas...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@esteseverino na verdade eles se comunicavam p...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>não tô chorando horrores com o final de la cas...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Teste  Relevante\n",
       "0  vou assistir a última temperada de la casa de ...          1\n",
       "1  só quero chegar em casa e me entupir de brigad...          1\n",
       "2  toda vez, a gnt depois que acaba de ver la cas...          0\n",
       "3  @esteseverino na verdade eles se comunicavam p...          1\n",
       "4  não tô chorando horrores com o final de la cas...          0"
      ]
     },
     "metadata": {},
     "execution_count": 39
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "___\n",
    "## Classificador automático de sentimento"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Nessa parte inicial do projeto é necessário categorizar a opinião daqueles comentários no twitter relativos à aprovação da série, (mais precisamente da nova temporada).\r\n",
    "\r\n",
    "Assim, foram encontrados diversos tweets, sendo alguns destes demasiadamente vagos, onde não dava para discernir se era um comentário aprovando ou não da série La casa de papel. Nesse caso, foi considerado relevante apenas os comentários que deixavam claro a critica à série. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- Funções utilizadas!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "source": [
    "# FUNÇÕES AUXILIARES DO SISTEMA para a montagem da base de dados do treinamento\r\n",
    "def cleanup(text):\r\n",
    "    \"\"\"\r\n",
    "        Função de limpeza muito simples que troca alguns sinais básicos por espaços\r\n",
    "    \"\"\"\r\n",
    "    #import string\r\n",
    "    punctuation = '[!-.:?;\\/)|,''\"\"“”@#(*]' # Note que os sinais [] são delimitadores de um conjunto.\r\n",
    "    pattern = re.compile(punctuation)\r\n",
    "    text_subbed = re.sub(pattern, '', text)\r\n",
    "    return text_subbed.lower()\r\n",
    "\r\n",
    "# função baseada no seguinte link: # https://stackoverflow.com/questions/49921720/how-to-split-emoji-from-each-other-python\r\n",
    "def separa_emoji(string):\r\n",
    "    em_split_emoji = emoji.get_emoji_regexp().split(string)\r\n",
    "    em_split_whitespace = [substr.split() for substr in em_split_emoji]\r\n",
    "    em_split = functools.reduce(operator.concat, em_split_whitespace)\r\n",
    "    return em_split\r\n",
    "\r\n",
    "# baixando as preposições\r\n",
    "nltk.download('stopwords')\r\n",
    "# definindo as preposições da lingua portuguesa\r\n",
    "stop = nltk.corpus.stopwords.words('portuguese')\r\n",
    "\r\n",
    "def segunda_limpeza(lis_tweet):\r\n",
    "    limpo = []\r\n",
    "    for pal in lis_tweet:\r\n",
    "        # além de verificar se a palavra não é uma preposição vamos pegar só as que tem mais de 3 letras\r\n",
    "        # vamos aproveitar que estamos percorrendo cada palavra e separar os emojis nesse processo\r\n",
    "        if pal not in stop:\r\n",
    "            aux = separa_emoji(pal)\r\n",
    "            for info in aux:\r\n",
    "                limpo.append(info)\r\n",
    "        else:\r\n",
    "            pass\r\n",
    "    return limpo\r\n",
    "\r\n",
    "def frase_para_palavras(df):\r\n",
    "    '''\r\n",
    "    função para percorrer linha a linha uma coluna do dataframe e concatenar as palavras em uma unica lista\r\n",
    "    para ser usada no value_counts()\r\n",
    "    '''\r\n",
    "    data = ''\r\n",
    "    \r\n",
    "    for tweet in df:\r\n",
    "        # o tweet é a frase que a pessoa postou, que se encontra em cada linha da nossa coluna\r\n",
    "        titulo = 'la casa de papel'\r\n",
    "        titulo_new = 'lacasadepapel'\r\n",
    "        if titulo in tweet:\r\n",
    "            tweet = tweet.replace(titulo,titulo_new)\r\n",
    "        data += tweet\r\n",
    "        \r\n",
    "    # dando o split agora, para separar por virgula\r\n",
    "    data = data.split()\r\n",
    "    # data é um vetor, podemos chamar a função de tirar preposição aqui\r\n",
    "    data = segunda_limpeza(data)\r\n",
    "    return data"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\gisel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "___\n",
    "\n",
    "\n",
    "## Montando um Classificador Naive-Bayes\n",
    "\n",
    "Considerando apenas as mensagens da planilha Treinamento, ensine  seu classificador."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "O classificador Naive-Bayes é da família de classificadores estatísticos baseado no Teorema de Bayes, sendo utilizado para prever a ocorrência em um texto, muito útil para esse trabalho já que estamos tratando uma sequência de tweets sobre o tema em questão \"La casa de papel\". \r\n",
    "\r\n",
    "Assim, esse classificador, ao ter um conhecimento prévio das condições - aqui dadas por relevante (1) e irrelevante (0) -  utiliza da frequência de palavras para fazer sua classificação, já que ele trata cada palavra de uma frase como um termo independente, como por exemplo: \"la casa de papel é muito bom\" é equivalente a \"é muito bom la casa de papel\".\r\n",
    "\r\n",
    "### O teorema de Bayes e as probabilidades independentes:\r\n",
    "\"Os eventos A e B são independentes quando o fato de ter conhecimento sobre a ocorrência de A não altera a expectativa sobre a probabilidade de ocorrência do evento B.\"\r\n",
    "\r\n",
    "Isso é muito útil para o nosso modelo já que a probabilidade de que as mesmas frases sejam ditas em postagens distintas no twitter é baixa, mas ao fazer a suposição de que cada palavra em uma frase atua de forma independente é possível se realizar os cálculos de probabilidade.\r\n",
    "\r\n",
    "### Cálculos de probabilidade\r\n",
    "\r\n",
    "- legenda:\r\n",
    "\r\n",
    "    - $P(rel)$: probabilidade do post ser relevante;\r\n",
    "\r\n",
    "    - $P(irrel)$: probabilidade do post ser irrelevante;\r\n",
    "\r\n",
    "    - $P(total)$: $P(rel)$ + $P(irrel)$\r\n",
    "\r\n",
    "    - $P(rel|total)$: \r\n",
    "    $$P(rel|total) = \\frac{P(total|rel) P(rel)}{P(total)}$$\r\n",
    "    \r\n",
    "    - $P(irrel|total)$: \r\n",
    "    $$P(irrel|total) = \\frac{P(total|irrel) P(irrel)}{P(total)}$$\r\n",
    "\r\n",
    "#### Suavização de Laplace\r\n",
    "Pelo cálculo das probabilidades independentes tem-se que, dado uma palavra, descobrir qual é a probabilidade de essa palavra está na base de dados a partir de sua frequência. Agora imagine que a palavra não está na base de dados... o que ocorre é que a probabilidade é zero, e ao realizar a multiplicação dos termos, o cálculo é anulado, não gerando nenhuma informação (o contrário do que se espera).\r\n",
    "\r\n",
    "Assim, a suavização de Laplace surge para prevenir esse caso, dado que é adicionado 1 a cada contagem, para que o denominador nunca fique nulo. Em contrapartida, é também adicionado o número de palavras totais ao divisor, o que leva a, ao realizar a divisão, gerar sempre um número menor que um. \r\n",
    "\r\n",
    "\r\n",
    "\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Apresentado o medelo, vamos implementar agora no código! "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "source": [
    "#convertendo em variáveis categóricas:\r\n",
    "train['Treinamento'] = train['Treinamento'].astype('category')\r\n",
    "test['Teste'] = test['Teste'].astype('category')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "source": [
    "#limpando pontuações do dataframe\r\n",
    "train['Clean']=train['Treinamento'].apply(cleanup)\r\n",
    "test['Clean']=test['Teste'].apply(cleanup)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "source": [
    "test"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Teste</th>\n",
       "      <th>Relevante</th>\n",
       "      <th>Clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>vou assistir a última temperada de la casa de ...</td>\n",
       "      <td>1</td>\n",
       "      <td>vou assistir a última temperada de la casa de ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>só quero chegar em casa e me entupir de brigad...</td>\n",
       "      <td>1</td>\n",
       "      <td>só quero chegar em casa e me entupir de brigad...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>toda vez, a gnt depois que acaba de ver la cas...</td>\n",
       "      <td>0</td>\n",
       "      <td>toda vez a gnt depois que acaba de ver la casa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@esteseverino na verdade eles se comunicavam p...</td>\n",
       "      <td>1</td>\n",
       "      <td>esteseverino na verdade eles se comunicavam pe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>não tô chorando horrores com o final de la cas...</td>\n",
       "      <td>0</td>\n",
       "      <td>não tô chorando horrores com o final de la cas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>acabei la casa de papel</td>\n",
       "      <td>0</td>\n",
       "      <td>acabei la casa de papel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>precisamos conversar sobre o final dessa tempo...</td>\n",
       "      <td>0</td>\n",
       "      <td>precisamos conversar sobre o final dessa tempo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>eu odeio lá casa de papel, mataram a tóquio va...</td>\n",
       "      <td>1</td>\n",
       "      <td>eu odeio lá casa de papel mataram a tóquio vai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>preguiça de ver essa parte 5 de la casa de papel</td>\n",
       "      <td>0</td>\n",
       "      <td>preguiça de ver essa parte 5 de la casa de papel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>la casa de papel não vai acabar nunca? não agu...</td>\n",
       "      <td>1</td>\n",
       "      <td>la casa de papel não vai acabar nunca não ague...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Teste  Relevante  \\\n",
       "0    vou assistir a última temperada de la casa de ...          1   \n",
       "1    só quero chegar em casa e me entupir de brigad...          1   \n",
       "2    toda vez, a gnt depois que acaba de ver la cas...          0   \n",
       "3    @esteseverino na verdade eles se comunicavam p...          1   \n",
       "4    não tô chorando horrores com o final de la cas...          0   \n",
       "..                                                 ...        ...   \n",
       "195                            acabei la casa de papel          0   \n",
       "196  precisamos conversar sobre o final dessa tempo...          0   \n",
       "197  eu odeio lá casa de papel, mataram a tóquio va...          1   \n",
       "198   preguiça de ver essa parte 5 de la casa de papel          0   \n",
       "199  la casa de papel não vai acabar nunca? não agu...          1   \n",
       "\n",
       "                                                 Clean  \n",
       "0    vou assistir a última temperada de la casa de ...  \n",
       "1    só quero chegar em casa e me entupir de brigad...  \n",
       "2    toda vez a gnt depois que acaba de ver la casa...  \n",
       "3    esteseverino na verdade eles se comunicavam pe...  \n",
       "4    não tô chorando horrores com o final de la cas...  \n",
       "..                                                 ...  \n",
       "195                            acabei la casa de papel  \n",
       "196  precisamos conversar sobre o final dessa tempo...  \n",
       "197  eu odeio lá casa de papel mataram a tóquio vai...  \n",
       "198   preguiça de ver essa parte 5 de la casa de papel  \n",
       "199  la casa de papel não vai acabar nunca não ague...  \n",
       "\n",
       "[200 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "execution_count": 43
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "source": [
    "# post relevante\r\n",
    "post_rel = train.loc[(train['Relevante'] == 1)]\r\n",
    "# post irrelevante\r\n",
    "post_irrel = train.loc[(train['Relevante'] == 0)]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "source": [
    "# agora precisamos quebrar o texto em palavras, usando a função aux que converter frase em palavras,\r\n",
    "# ao mesmo tempo, vamos retirar as preposições do código, paralelamente a conversão, melhorando a base de dados, temos:\r\n",
    "data_rel   = frase_para_palavras(post_rel['Clean'])\r\n",
    "data_irrel = frase_para_palavras(post_irrel['Clean'])\r\n",
    "\r\n",
    "# criando a serie dos posts relevantes e irrelevantes\r\n",
    "serie_rel = pd.Series(data_rel)\r\n",
    "serie_irrel = pd.Series(data_irrel)\r\n",
    "# Formando uma base de daddos com todas as palavras nele:\r\n",
    "data_total = data_rel + data_irrel\r\n",
    "\r\n",
    "#criando a serie com a base total de dados\r\n",
    "serie_total = pd.Series(data_total)\r\n",
    "\r\n",
    "print(len(serie_rel))\r\n",
    "print(len(serie_irrel))\r\n",
    "print(len(serie_total))\r\n",
    "serie_irrel.tail(20)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1368\n",
      "1087\n",
      "2455\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1067        trabalhar\n",
       "1068               🤦🏾\n",
       "1069               🤦🏾\n",
       "1070           recebi\n",
       "1071          spoiler\n",
       "1072    lacasadepapel\n",
       "1073            quero\n",
       "1074          cometer\n",
       "1075     homicídiovou\n",
       "1076       aproveitar\n",
       "1077              pra\n",
       "1078              ver\n",
       "1079            novos\n",
       "1080        episódios\n",
       "1081               lá\n",
       "1082             casa\n",
       "1083            papel\n",
       "1084       abafadinha\n",
       "1085               🙏🏼\n",
       "1086                ✨\n",
       "dtype: object"
      ]
     },
     "metadata": {},
     "execution_count": 45
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Frequências absolutas"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Palavras em um texto são variáveis **qualitativas nominais**, portanto usaremos `value_counts()` para obter a tabela de frequências relativas e absolutas:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "source": [
    "# construindo a tabela de relevante e irrelevante\r\n",
    "tabela_rel   = serie_rel.value_counts()\r\n",
    "tabela_irrel = serie_irrel.value_counts()\r\n",
    "tabela_total = serie_total.value_counts()\r\n",
    "lista_serie_total = serie_total.tolist()\r\n",
    "elementos_nao_repetidos = set(lista_serie_total)\r\n",
    "elementos_nao_repetidos = pd.DataFrame(elementos_nao_repetidos).value_counts()\r\n",
    "elementos_nao_repetidos"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "🥺                      1\n",
       "fdp                    1\n",
       "favs                   1\n",
       "faz                    1\n",
       "fazendo                1\n",
       "                      ..\n",
       "parece                 1\n",
       "parar                  1\n",
       "parabénslarissajmar    1\n",
       "papelmó                1\n",
       "100                    1\n",
       "Length: 1143, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 46
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "___\n",
    "### Verificando a performance do Classificador\n",
    "\n",
    "Agora você deve testar o seu classificador com a base de Testes."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "source": [
    "# postagens relevantes\r\n",
    "post_rel.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Treinamento</th>\n",
       "      <th>Relevante</th>\n",
       "      <th>Clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>coisas q serão proibidas quando eu for preside...</td>\n",
       "      <td>1</td>\n",
       "      <td>coisas q serão proibidas quando eu for preside...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>agr que tô terminando de assistir la casa de p...</td>\n",
       "      <td>1</td>\n",
       "      <td>agr que tô terminando de assistir la casa de p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>olha, quem me segue aqui sabe o tanto que odei...</td>\n",
       "      <td>1</td>\n",
       "      <td>olha quem me segue aqui sabe o tanto que odeio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>que final fdp do lá casa de papel</td>\n",
       "      <td>1</td>\n",
       "      <td>que final fdp do lá casa de papel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>final veio de lá casa de papel, meu pai amado ...</td>\n",
       "      <td>1</td>\n",
       "      <td>final veio de lá casa de papel meu pai amado o...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Treinamento  Relevante  \\\n",
       "0  coisas q serão proibidas quando eu for preside...          1   \n",
       "1  agr que tô terminando de assistir la casa de p...          1   \n",
       "4  olha, quem me segue aqui sabe o tanto que odei...          1   \n",
       "6                  que final fdp do lá casa de papel          1   \n",
       "8  final veio de lá casa de papel, meu pai amado ...          1   \n",
       "\n",
       "                                               Clean  \n",
       "0  coisas q serão proibidas quando eu for preside...  \n",
       "1  agr que tô terminando de assistir la casa de p...  \n",
       "4  olha quem me segue aqui sabe o tanto que odeio...  \n",
       "6                  que final fdp do lá casa de papel  \n",
       "8  final veio de lá casa de papel meu pai amado o...  "
      ]
     },
     "metadata": {},
     "execution_count": 47
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "source": [
    "# postagens irrelevantes\r\n",
    "post_irrel.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Treinamento</th>\n",
       "      <th>Relevante</th>\n",
       "      <th>Clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>paguem minha terapia (la casa de papel vc me p...</td>\n",
       "      <td>0</td>\n",
       "      <td>paguem minha terapia la casa de papel vc me paga</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>to com pena de terminar la casa de papel</td>\n",
       "      <td>0</td>\n",
       "      <td>to com pena de terminar la casa de papel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>simplesmente devastada com esse final da 1° pa...</td>\n",
       "      <td>0</td>\n",
       "      <td>simplesmente devastada com esse final da 1° pa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ai la casa de papel me mata pqp</td>\n",
       "      <td>0</td>\n",
       "      <td>ai la casa de papel me mata pqp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>criadores de lá casa de papel deixaram mto na ...</td>\n",
       "      <td>0</td>\n",
       "      <td>criadores de lá casa de papel deixaram mto na ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Treinamento  Relevante  \\\n",
       "2   paguem minha terapia (la casa de papel vc me p...          0   \n",
       "3            to com pena de terminar la casa de papel          0   \n",
       "5   simplesmente devastada com esse final da 1° pa...          0   \n",
       "7                     ai la casa de papel me mata pqp          0   \n",
       "10  criadores de lá casa de papel deixaram mto na ...          0   \n",
       "\n",
       "                                                Clean  \n",
       "2    paguem minha terapia la casa de papel vc me paga  \n",
       "3            to com pena de terminar la casa de papel  \n",
       "5   simplesmente devastada com esse final da 1° pa...  \n",
       "7                     ai la casa de papel me mata pqp  \n",
       "10  criadores de lá casa de papel deixaram mto na ...  "
      ]
     },
     "metadata": {},
     "execution_count": 48
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "source": [
    "p_rel = len(serie_rel)/len(serie_total)\r\n",
    "print('A probabilidade de relevantes é: {0:.3f}'.format(p_rel))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "A probabilidade de relevantes é: 0.557\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "source": [
    "p_irrel = len(serie_irrel)/len(serie_total)\r\n",
    "print('A probabilidade de irrelevate é: {0:.3f}'.format(p_irrel))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "A probabilidade de irrelevate é: 0.443\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Funções auxiliares para a suavização de la place"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "source": [
    "def quantas_vezes_aparece_palavra(palavra,tabela):\r\n",
    "    # a função retorna o valor de vezes que a palavra aparece no seu conjunto\r\n",
    "    # por exemplo, quantas vezes aparece na tabela relevante?\r\n",
    "    # fazemos: num = tabela_rel['palavra']\r\n",
    "    # e retornamos esse valor\r\n",
    "    # por outro lado, pode não ter a palavra, por isso a suaviação de la place, quando isso ocorrer,\r\n",
    "    # vamos retornar 0\r\n",
    "    try:\r\n",
    "        # caso em que a palavra tem na tabela de frequencias\r\n",
    "        quant = tabela[palavra]\r\n",
    "        return quant\r\n",
    "    except:\r\n",
    "        # aqui a palavra não está na tabela de frequencias,retornamos 0\r\n",
    "        return 0\r\n",
    "    \r\n",
    "def laplace_smoothing(palavras_da_classe,quantidade):\r\n",
    "    # vamos aplicar a formula da suavização\r\n",
    "    num = quantidade + 1\r\n",
    "    den = len(palavras_da_classe) + len(elementos_nao_repetidos)\r\n",
    "    #den = 2193 + len(elementos_nao_repetidos)\r\n",
    "    laplace = num/den\r\n",
    "    return laplace\r\n",
    "\r\n",
    "def p_tweet(tweet,relevante):\r\n",
    "    # se relevante for true, calcula para relevante\r\n",
    "    # se for false, para irrelevante\r\n",
    "    p = 1\r\n",
    "    # quebrando o tweet a cada espaço \r\n",
    "    tweet_em_palavras = tweet.split()\r\n",
    "    \r\n",
    "    for palavra in tweet_em_palavras:\r\n",
    "        # vamos calcular o valor da frequencia absoluta da palavra\r\n",
    "        # para isto, usaremos a funcao quantas_vezes_aparece_palavra\r\n",
    "        # verificando qual classe queremos calcular p\r\n",
    "        \r\n",
    "        if relevante:\r\n",
    "            quant = quantas_vezes_aparece_palavra(palavra,tabela_rel)\r\n",
    "            p_palavra = laplace_smoothing(serie_rel,quant)\r\n",
    "            \r\n",
    "        else:\r\n",
    "            quant = quantas_vezes_aparece_palavra(palavra,tabela_irrel)\r\n",
    "            p_palavra = laplace_smoothing(serie_irrel,quant)\r\n",
    "            \r\n",
    "        p *= p_palavra\r\n",
    "    return p\r\n",
    "\r\n",
    "def rel_ou_irrel(tweet):\r\n",
    "    # vamos decidir se o tweet é relevante ou não\r\n",
    "    # para tal vamos aplicar o teorema de bayes\r\n",
    "    P_tweet_rel   = p_tweet(tweet,True)\r\n",
    "    P_tweet_irrel = p_tweet(tweet,False)\r\n",
    "    rel_bayes     = P_tweet_rel*p_rel\r\n",
    "    irrel_bayes   = P_tweet_irrel*p_irrel\r\n",
    "    #print(rel_bayes,irrel_bayes)\r\n",
    "    # vamos usar o mesmo criterio adotado na tabela, 1 para relevante e 0 para irrelevante\r\n",
    "    if rel_bayes > irrel_bayes:\r\n",
    "        return 1\r\n",
    "    else:\r\n",
    "        return 0"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "source": [
    "# agora vamos criar uma coluna nova, com a classificação dos tweets\r\n",
    "train['Classificação'] = train['Clean'].apply(rel_ou_irrel)\r\n",
    "train"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Treinamento</th>\n",
       "      <th>Relevante</th>\n",
       "      <th>Clean</th>\n",
       "      <th>Classificação</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>coisas q serão proibidas quando eu for preside...</td>\n",
       "      <td>1</td>\n",
       "      <td>coisas q serão proibidas quando eu for preside...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>agr que tô terminando de assistir la casa de p...</td>\n",
       "      <td>1</td>\n",
       "      <td>agr que tô terminando de assistir la casa de p...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>paguem minha terapia (la casa de papel vc me p...</td>\n",
       "      <td>0</td>\n",
       "      <td>paguem minha terapia la casa de papel vc me paga</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>to com pena de terminar la casa de papel</td>\n",
       "      <td>0</td>\n",
       "      <td>to com pena de terminar la casa de papel</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>olha, quem me segue aqui sabe o tanto que odei...</td>\n",
       "      <td>1</td>\n",
       "      <td>olha quem me segue aqui sabe o tanto que odeio...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>mó sacanagem esse final de la casa de papel</td>\n",
       "      <td>1</td>\n",
       "      <td>mó sacanagem esse final de la casa de papel</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>recebi spoiler de la casa de papel e quero com...</td>\n",
       "      <td>0</td>\n",
       "      <td>recebi spoiler de la casa de papel e quero com...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>terminei la casa de papel e me tornei o homem ...</td>\n",
       "      <td>1</td>\n",
       "      <td>terminei la casa de papel e me tornei o homem ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>vou aproveitar pra ver os novos episódios de l...</td>\n",
       "      <td>0</td>\n",
       "      <td>vou aproveitar pra ver os novos episódios de l...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>terminei de assistir lá casa de papel ,alguém ...</td>\n",
       "      <td>1</td>\n",
       "      <td>terminei de assistir lá casa de papel alguém p...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>300 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Treinamento  Relevante  \\\n",
       "0    coisas q serão proibidas quando eu for preside...          1   \n",
       "1    agr que tô terminando de assistir la casa de p...          1   \n",
       "2    paguem minha terapia (la casa de papel vc me p...          0   \n",
       "3             to com pena de terminar la casa de papel          0   \n",
       "4    olha, quem me segue aqui sabe o tanto que odei...          1   \n",
       "..                                                 ...        ...   \n",
       "295        mó sacanagem esse final de la casa de papel          1   \n",
       "296  recebi spoiler de la casa de papel e quero com...          0   \n",
       "297  terminei la casa de papel e me tornei o homem ...          1   \n",
       "298  vou aproveitar pra ver os novos episódios de l...          0   \n",
       "299  terminei de assistir lá casa de papel ,alguém ...          1   \n",
       "\n",
       "                                                 Clean  Classificação  \n",
       "0    coisas q serão proibidas quando eu for preside...              1  \n",
       "1    agr que tô terminando de assistir la casa de p...              1  \n",
       "2     paguem minha terapia la casa de papel vc me paga              0  \n",
       "3             to com pena de terminar la casa de papel              0  \n",
       "4    olha quem me segue aqui sabe o tanto que odeio...              1  \n",
       "..                                                 ...            ...  \n",
       "295        mó sacanagem esse final de la casa de papel              1  \n",
       "296  recebi spoiler de la casa de papel e quero com...              0  \n",
       "297  terminei la casa de papel e me tornei o homem ...              1  \n",
       "298  vou aproveitar pra ver os novos episódios de l...              0  \n",
       "299  terminei de assistir lá casa de papel alguém p...              1  \n",
       "\n",
       "[300 rows x 4 columns]"
      ]
     },
     "metadata": {},
     "execution_count": 52
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "source": [
    "test['Classificação'] = test['Clean'].apply(rel_ou_irrel)\r\n",
    "test"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Teste</th>\n",
       "      <th>Relevante</th>\n",
       "      <th>Clean</th>\n",
       "      <th>Classificação</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>vou assistir a última temperada de la casa de ...</td>\n",
       "      <td>1</td>\n",
       "      <td>vou assistir a última temperada de la casa de ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>só quero chegar em casa e me entupir de brigad...</td>\n",
       "      <td>1</td>\n",
       "      <td>só quero chegar em casa e me entupir de brigad...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>toda vez, a gnt depois que acaba de ver la cas...</td>\n",
       "      <td>0</td>\n",
       "      <td>toda vez a gnt depois que acaba de ver la casa...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@esteseverino na verdade eles se comunicavam p...</td>\n",
       "      <td>1</td>\n",
       "      <td>esteseverino na verdade eles se comunicavam pe...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>não tô chorando horrores com o final de la cas...</td>\n",
       "      <td>0</td>\n",
       "      <td>não tô chorando horrores com o final de la cas...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>acabei la casa de papel</td>\n",
       "      <td>0</td>\n",
       "      <td>acabei la casa de papel</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>precisamos conversar sobre o final dessa tempo...</td>\n",
       "      <td>0</td>\n",
       "      <td>precisamos conversar sobre o final dessa tempo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>eu odeio lá casa de papel, mataram a tóquio va...</td>\n",
       "      <td>1</td>\n",
       "      <td>eu odeio lá casa de papel mataram a tóquio vai...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>preguiça de ver essa parte 5 de la casa de papel</td>\n",
       "      <td>0</td>\n",
       "      <td>preguiça de ver essa parte 5 de la casa de papel</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>la casa de papel não vai acabar nunca? não agu...</td>\n",
       "      <td>1</td>\n",
       "      <td>la casa de papel não vai acabar nunca não ague...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Teste  Relevante  \\\n",
       "0    vou assistir a última temperada de la casa de ...          1   \n",
       "1    só quero chegar em casa e me entupir de brigad...          1   \n",
       "2    toda vez, a gnt depois que acaba de ver la cas...          0   \n",
       "3    @esteseverino na verdade eles se comunicavam p...          1   \n",
       "4    não tô chorando horrores com o final de la cas...          0   \n",
       "..                                                 ...        ...   \n",
       "195                            acabei la casa de papel          0   \n",
       "196  precisamos conversar sobre o final dessa tempo...          0   \n",
       "197  eu odeio lá casa de papel, mataram a tóquio va...          1   \n",
       "198   preguiça de ver essa parte 5 de la casa de papel          0   \n",
       "199  la casa de papel não vai acabar nunca? não agu...          1   \n",
       "\n",
       "                                                 Clean  Classificação  \n",
       "0    vou assistir a última temperada de la casa de ...              1  \n",
       "1    só quero chegar em casa e me entupir de brigad...              0  \n",
       "2    toda vez a gnt depois que acaba de ver la casa...              0  \n",
       "3    esteseverino na verdade eles se comunicavam pe...              0  \n",
       "4    não tô chorando horrores com o final de la cas...              1  \n",
       "..                                                 ...            ...  \n",
       "195                            acabei la casa de papel              1  \n",
       "196  precisamos conversar sobre o final dessa tempo...              1  \n",
       "197  eu odeio lá casa de papel mataram a tóquio vai...              1  \n",
       "198   preguiça de ver essa parte 5 de la casa de papel              0  \n",
       "199  la casa de papel não vai acabar nunca não ague...              0  \n",
       "\n",
       "[200 rows x 4 columns]"
      ]
     },
     "metadata": {},
     "execution_count": 53
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "source": [
    "# verdadeiro_negativo = test.loc[(test['Relevante']==0)&(test['Classificação']==0),:].shape[0]\r\n",
    "# verdadeiro_positivo = test.loc[(test['Relevante']==1)&(test['Classificação']==1),:].shape[0]\r\n",
    "# acuracia = (verdadeiro_positivo + verdadeiro_negativo)/test.shape[0]\r\n",
    "# print('A acuracia do classificador na base de teste foi de {:.2f} %'.format(acuracia*100))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "source": [
    "# # analizando a classificaçao como um todo, e poltando um grafico, temos:\r\n",
    "# df_cm=pd.crosstab(test.Relevante,test.Classificação,normalize=True)\r\n",
    "# plt.figure(figsize = (9,7))\r\n",
    "# plt.title('Matriz de Confusão na Base Treino \\n informações em porcentagem\\n',fontdict={'fontsize': 14})\r\n",
    "# sn.heatmap(df_cm, annot=True, annot_kws={\"size\":30},fmt='.1%',cmap=\"BuPu\")"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Deste modo, temos: \n",
    "14   % de falso positivo\n",
    "16,5 % de verdadeiro positivo\n",
    "23   % de falso negativo\n",
    "46,5 % de verdadeiro negativo"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "source": [
    "#porcentagem de verdadeiros negativos:\r\n",
    "relevante = test.loc[test['Relevante']==0]\r\n",
    "classificacao_rel = relevante.loc[relevante['Classificação']==0]\r\n",
    "#print('total de verdadeiros negativos:{0}'.format(len(classificacao_rel)))\r\n",
    "print('A porcentagem de verdadeiros negativos: {:.4}%'.format( ((len(classificacao_rel))/len(test['Relevante'])*100)))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "A porcentagem de verdadeiros negativos: 46.0%\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "source": [
    "#porcentagem de verdadeiros positivos:\r\n",
    "irrelevante = test.loc[test['Relevante']==1]\r\n",
    "classificacao_irrel = irrelevante.loc[irrelevante['Classificação']==1]\r\n",
    "\r\n",
    "print('A porcentagem de verdadeiros positivos: {:.4}%'.format( ((len(classificacao_irrel))/len(test['Relevante'])*100)))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "A porcentagem de verdadeiros positivos: 17.5%\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "source": [
    "#porcentagem de falsos negativos:\r\n",
    "relevante = test.loc[test['Relevante']==1]\r\n",
    "classificacao_irrel = relevante.loc[relevante['Classificação']==0]\r\n",
    "print('A porcentagem de falsos negativos é: {:.4}%'.format(((len(classificacao_irrel))/len(test['Relevante'])*100)))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "A porcentagem de falsos negativos é: 13.0%\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "source": [
    "#porcentagem de falsos positivo:\r\n",
    "irrelevante = test.loc[test['Relevante']==0]\r\n",
    "classificacao_rel = irrelevante.loc[irrelevante['Classificação']==1]\r\n",
    "print('a porcentagem de falsos positivos é: {:.4}%'.format( ((len(classificacao_rel))/len(test['Relevante'])*100)))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "a porcentagem de falsos positivos é: 23.5%\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "source": [
    "#acurácia\r\n",
    "total_irrel = len(test.loc[(test['Relevante']==0)&(test['Classificação']==0)])\r\n",
    "total_rel = len(test.loc[(test['Relevante']==1)&(test['Classificação']==1)])\r\n",
    "\r\n",
    "total = total_rel + total_irrel\r\n",
    "acuracia = total/len(test['Relevante'])\r\n",
    "acuracia *= 100\r\n",
    "print('A acurácia é: {:.4} % '.format(acuracia))\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "A acurácia é: 63.5 % \n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "source": [
    "#Vamos fazer a acurácia da base de dados de treinamento para análises conclusivas:\r\n",
    "total_irrel_train = len(train.loc[(train['Relevante']==0)&(train['Classificação']==0)])\r\n",
    "total_rel_train = len(train.loc[(train['Relevante']==1)&(train['Classificação']==1)])\r\n",
    "\r\n",
    "total_train = total_rel_train + total_irrel_train\r\n",
    "acuracia_train = total/len(train['Relevante'])\r\n",
    "acuracia_train *= 100\r\n",
    "print('A acurácia é: {:.4} % '.format(acuracia_train))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "A acurácia é: 42.33 % \n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "___\r\n",
    "## Concluindo"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Interpretações do modelo:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "A partir da montagem do classificador é possível então fazer análises sobre a execução do nosso modelo probabilístico. Assim, com uma acurácia de 63.5% pode-se interpretar que ele teve um desempenho razoável, por mais que ainda insatisfatório dados que ainda representa menos de 75% de acerto. Isso significa que, após ele ter feito a sua classificação, quando comparado com a base de dados classificada manualmente, ele obteve 63.5% de tweets com classificações coerentes com a realidade.  \r\n",
    "\r\n",
    "Isso se deve, sobretudo, a classificação dos tweets falsos positivos (aqueles que foram demarcados como relevantes, mas que na verdade são irrelevantes), sendo 23.5% em nosso modelo e dos falsos negativos (aqueles demarcados como irrelevantes, mas que na verdade são relevantes), com 13% em nosso modelo. \r\n",
    "\r\n",
    "Uma forma de explicar isso é pelo fato de se utilizar probabilidades independentes para classificação. Ou seja, os tweets são classificados apenas pela probabilidade independente de cada palavra ser classificada como relevante ou irrelevante, ignorando a interpretação das frases como um todo, deixando de lado o seu sentido. Assim, frases ironicas, sarcásticas, ambíguas ou com dupla negação são um risco para o nosso modelo. \r\n",
    "\r\n",
    "Apesar disso, o tamanho da base de dados influencia diretamente na qualidade de desempenho do classificador, já que quanto mais postagens armazenadas na base de dados, maior é a assimilação de palavras e maior é a quantidade de palavras que podem ser usadas em postagens posteriores. Dado isso, você pode se perguntar <em><b> \"Seria interessante então utilizar o próprio classificador para gerar mais amostras de treinamento\" </em></b> e a resposta para isso é <em><b>\"não\"</em></b>, de forma direta, o algorítmo toma como base a certeza de que as classificações da base de dados de treinamento é 100% correta e constrói o seu classificador (que no caso do projeto possui uma probabilidade de errar em 36,5% das cassificações)  assim, se para futuras classificações for utilizado tweets classificados erroneamente, ele fará classificações cada vez mais erradas. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Onde é possível aplicar o Naïve Bayes fora do contexto desse projeto?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Tendo em vista "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "De acordo com o artigo \"Bazinga! Caracterizando e Detectando Sarcasmo e Ironia no Twitter\" </em></b>  da Universidade Federal de Minas gerais, percebemos que o tweet contendo sarcasmo faz uso significativo de palavras que expressam concordância, ainda que de maneira mais informal, com uso de expressões como “Er”, “hm”e “umm”. Outro aspecto observado está relacionado ao uso intenso da pontuac¸ ˜ao dois pontos (“:”), que é utilizada para o anúncio ou introdução de um esclarecimento ou citação. Já o conteúdo irônico tem presença significativa de pronomes na terceira pessoa do plural. Estudos anteriores indicam que essas são características de uma linguagem mais informal."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<center> <img src=\"imagens/Tabela de ironia.png\" width=500> <center>\r\n",
    "foto retirada do artigo <em><b> \"Bazinga! Caracterizando e Detectando Sarcasmo e Ironia no Twitter\" </em></b>  da Universidade Federal de Minas gerais"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "___\n",
    "### Qualidade do Classificador a partir de novas separações dos tweets entre Treinamento e Teste\n",
    "\n",
    "Caso for fazer esse item do Projeto"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "source": [
    "# vamos juntar as duas tabelas em uma tabela só\r\n",
    "test_novo = pd.read_excel(filename, sheet_name = 'Teste')\r\n",
    "test_novo = test_novo.rename(columns={'Teste':'Total'})\r\n",
    "\r\n",
    "train_novo = pd.read_excel(filename, sheet_name = 'Treinamento')\r\n",
    "train_novo = train_novo.rename(columns={'Treinamento':'Total'})\r\n",
    "\r\n",
    "# juntando as duas tabelas\r\n",
    "data_total_ = pd.concat([train_novo,test_novo])\r\n",
    "# limpando a pontuação\r\n",
    "# a limpeza mais profunda ocorrerá quando transfprmamos os tweets em palavras, neste passo, iremos limpar, separar emojis, tudo\r\n",
    "# paralelamente\r\n",
    "data_total_['Clean'] = data_total_['Total'].apply(cleanup)\r\n",
    "data_total_\r\n",
    "type(data_total_)\r\n",
    "# agora devemos aplicar a rotina de limpeza novamente"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "metadata": {},
     "execution_count": 62
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "source": [
    "# https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\r\n",
    "# vamos usar a biblioteca sklearn.model_selection.train_test_split, com código de exemplo obtido no link acima\r\n",
    "\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "# vamos guardar os verdadeiros positivos, verdadeiros negativos e a acuracia de cada rodagem em um vetor para analiza grafica futura\r\n",
    "true_positive = []\r\n",
    "true_negative = []\r\n",
    "acurracy      = []\r\n",
    "\r\n",
    "# queremos repetir pelo menos 100 vezes, para repetir n vezes coloque no segundo argumento no range n-1\r\n",
    "for i in range(0,100):\r\n",
    "    #print(type(data_total))\r\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data_total_[['Clean','Relevante']],data_total_['Relevante'], test_size=0.4)\r\n",
    "    # teste size é a proporção da base de dados para teste. Como queremos 200/500, teste sieze será 0.4\r\n",
    "    #print('amigo estou aq')\r\n",
    "    # separando o treinamento relevante do irrelevante\r\n",
    "    train_relevantes   = X_train[X_train['Relevante'] == 1]\r\n",
    "    train_irrelevantes = X_train[X_train['Relevante'] == 0]\r\n",
    "    \r\n",
    "    # agora vamos quebrar o texto em palavras, e fazer a segunda limpeza nos dados, desde considerar 'la casa de papel'\r\n",
    "    # como 'lacasadepapel' a separar os emojis. A FUNÇÃO FRASE_PARA_PALAVRAS fara a limpeza citada\r\n",
    "    \r\n",
    "    data_rel   = frase_para_palavras(train_relevantes['Clean'])\r\n",
    "    data_irrel = frase_para_palavras(train_irrelevantes['Clean'])\r\n",
    "    \r\n",
    "    # Formando uma base de daddos com todas as palavras nele:\r\n",
    "    data_total = data_rel + data_irrel\r\n",
    "    \r\n",
    "    # criando a serie dos posts relevantes e irrelevantes\r\n",
    "    serie_rel = pd.Series(data_rel)\r\n",
    "    serie_irrel = pd.Series(data_irrel)\r\n",
    "    #criando a serie com a base total de dados\r\n",
    "    serie_total = pd.Series(data_total)\r\n",
    "    \r\n",
    "    # criando tabela de frequencia das palavras, seram usadas para calcular a frequencia absoluta na suavização de laplace\r\n",
    "    tabela_rel   = serie_rel.value_counts()\r\n",
    "    tabela_irrel = serie_irrel.value_counts()\r\n",
    "    tabela_total = serie_total.value_counts()\r\n",
    "    # vamos calcular os elementos não repetidos, necessário na suavização de laplace\r\n",
    "    lista_serie_total = serie_total.tolist()\r\n",
    "    elementos_nao_repetidos = set(lista_serie_total)\r\n",
    "    elementos_nao_repetidos = pd.DataFrame(elementos_nao_repetidos).value_counts()\r\n",
    "    \r\n",
    "    # definindo a probabilidade de ser relevante e a probabilidade de ser irrelevante\r\n",
    "    p_rel = len(serie_rel)/len(serie_total)\r\n",
    "    p_irrel = len(serie_irrel)/len(serie_total)\r\n",
    "    \r\n",
    "    # neste momento vamos classificar a base de teste\r\n",
    "    X_test['Classificação'] = X_test['Clean'].apply(rel_ou_irrel)\r\n",
    "    \r\n",
    "    # agora vamos calular porcentagem dos verdadeiros positivos, verdadeiros negativos e a acuracia do sistema\r\n",
    "    verdadeiro_positivo=X_test.loc[(X_test['Classificação']==1)&(X_test['Relevante']==1),:].shape[0]\r\n",
    "    verdadeiro_negativo=X_test.loc[(X_test['Classificação']==0)&(X_test['Relevante']==0),:].shape[0]\r\n",
    "    acuracia=(verdadeiro_positivo+verdadeiro_negativo)/X_test.shape[0]\r\n",
    "    \r\n",
    "    # calculando as porcentagens:\r\n",
    "    P_ver_pos = (verdadeiro_positivo/X_test.shape[0])*100\r\n",
    "    P_ver_neg = (verdadeiro_negativo/X_test.shape[0])*100\r\n",
    "    acuracia *= 100\r\n",
    "    \r\n",
    "    # guardando os verdadeiros positivos, negativos e a acuraria no vetor vazia criando antes do laço de repetição\r\n",
    "    true_positive.append(P_ver_pos)\r\n",
    "    true_negative.append(P_ver_neg)\r\n",
    "    acurracy.append(acuracia)    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "source": [
    "faixa=np.arange(30,99,1)\r\n",
    "plt.figure(figsize=(10, 5))\r\n",
    "plt.hist(acurracy, bins=faixa, edgecolor='black', density=False)\r\n",
    "plt.title('Frequencia dos scores do modelo')\r\n",
    "plt.ylabel('Frequência')\r\n",
    "plt.xlabel('Porcentagem de acerto (%)')\r\n",
    "plt.xlim(52.5,72.5)\r\n",
    "plt.show()"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "___\n",
    "## Aperfeiçoamento:\n",
    "\n",
    "Trabalhos que conseguirem pelo menos conceito B vão evoluir em conceito dependendo da quantidade de itens avançados:\n",
    "\n",
    "* IMPLEMENTOU outras limpezas e transformações que não afetem a qualidade da informação contida nos tweets. Ex: stemming, lemmatization, stopwords\n",
    "* CORRIGIU separação de espaços entre palavras e emojis ou entre emojis e emojis\n",
    "* CRIOU categorias intermediárias de relevância baseadas na probabilidade: ex.: muito relevante, relevante, neutro, irrelevante, muito irrelevante. Pelo menos quatro categorias, com adição de mais tweets na base, conforme enunciado. (OBRIGATÓRIO PARA TRIOS, sem contar como item avançado)\n",
    "* EXPLICOU porquê não pode usar o próprio classificador para gerar mais amostras de treinamento\n",
    "* PROPÔS diferentes cenários para Naïve Bayes fora do contexto do projeto\n",
    "* SUGERIU e EXPLICOU melhorias reais com indicações concretas de como implementar (indicar como fazer e indicar material de pesquisa)\n",
    "* FEZ o item 6. Qualidade do Classificador a partir de novas separações dos tweets entre Treinamento e Teste descrito no enunciado do projeto (OBRIGATÓRIO para conceitos A ou A+)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "___\n",
    "## Referências"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "[Naive Bayes and Text Classification](https://arxiv.org/pdf/1410.5329.pdf)  **Mais completo**\n",
    "\n",
    "[A practical explanation of a Naive Bayes Classifier](https://monkeylearn.com/blog/practical-explanation-naive-bayes-classifier/) **Mais simples**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a036e081e21c160afc26bb4b03f978afee4695dfefad222ea89752b41a5783e0"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}